# どうぶつしょうぎAIパラメータ調整 教育的コンテンツ企画書

## 1. 実験的な遊び方（シナリオ）【メインコンテンツ】

### 1.1 近視眼AI（うっかり屋）を作る実験

**目的：** 探索深さがAIの判断に与える影響を体験する

**実験手順：**
1. AI選択画面で「2分木AI」または「最強AI」を選択
2. パラメータ調整画面で「探索深さ」を最小値（2または3）に設定
3. 対局を開始し、AIの手を観察する

**期待される変化：**
- AIが短期的な利益（駒を取る）に走りやすくなる
- 長期的な戦略（ライオンを守る、成りを狙う）を見落としやすくなる
- 「うっかり」相手のライオンを取られてしまう場面が増える

**観察ポイント：**
- AIが明らかに不利な手を選ぶ場面があるか
- 何手先まで読んでいるように見えるか
- 探索深さを1増やすと、AIの手がどう変わるか

**発展実験：**
- 探索深さを1→2→3→4と段階的に増やし、AIの強さの変化を記録する
- 同じ局面で異なる探索深さのAIに手を選ばせ、選択の違いを比較する

---

### 1.2 偏愛AI（特定の駒を溺愛）を作る実験

**目的：** 評価関数における駒の価値設定がAIの行動に与える影響を理解する

**実験手順：**
1. AI選択画面で「ディープラーニングAI」を選択
2. パラメータ調整画面で「駒の価値」を調整
3. 極端な設定を試す（例：ひよこの価値を10,000に設定）

**標準設定（初期値）：**
- ライオン: 10,000
- にわとり: 600
- きりん: 400
- ぞう: 400
- ひよこ: 100

**実験パターン例：**

#### パターンA：ひよこ偏愛AI
- ひよこ: 10,000（ライオンと同じ）
- ライオン: 1,000
- その他: 標準値

**期待される変化：**
- AIがひよこを過度に守ろうとする
- ライオンを取るより、ひよこを守ることを優先する
- ひよこの成り（にわとり）を極端に重視する

#### パターンB：ライオン軽視AI
- ライオン: 100（ひよこと同じ）
- ひよこ: 10,000
- その他: 標準値

**期待される変化：**
- AIがライオンを簡単に捨てる
- ライオンを取られても平気な顔をする（評価値が下がらない）
- ひよこを最優先で守る

#### パターンC：きりん・ぞう偏愛AI
- きりん: 5,000
- ぞう: 5,000
- ライオン: 1,000
- その他: 標準値

**期待される変化：**
- きりんとぞうを過度に守る
- ライオンよりきりん・ぞうを優先する場面が増える

**観察ポイント：**
- AIがどの駒を優先的に守るか
- どの駒を取ることを優先するか
- 評価値の表示と実際の行動が一致しているか

**発展実験：**
- 駒の価値を段階的に変更し、AIの行動が変わる「閾値」を探す
- 複数の偏愛パターンを試し、どれが最も強い（または弱い）かを比較する

---

### 1.3 確率の気まぐれを観察する実験

**目的：** モンテカルロ法の試行回数がAIの思考の「揺らぎ」に与える影響を理解する

**実験手順：**
1. AI選択画面で「モンテカルロAI」（将来実装予定）を選択
2. パラメータ調整画面で「試行回数」を調整
3. 同じ局面で複数回対局し、AIの手の選択を観察する

**実験パターン：**

#### パターンA：低試行回数（100回）
- 試行回数: 100
- 期待される変化：AIの手が毎回異なる可能性が高い（揺らぎが大きい）

#### パターンB：中試行回数（1,000回）
- 試行回数: 1,000
- 期待される変化：AIの手がある程度安定するが、たまに異なる手を選ぶ

#### パターンC：高試行回数（10,000回）
- 試行回数: 10,000
- 期待される変化：AIの手がほぼ毎回同じ（揺らぎが小さい）

**観察ポイント：**
- 同じ局面でAIが何回異なる手を選ぶか
- 試行回数を増やすと、AIの手がどう変わるか
- 「気まぐれ」な手が良い結果をもたらすことがあるか

**発展実験：**
- 同じ局面を10回再現し、AIが選ぶ手の分布を記録する
- 試行回数と思考時間の関係をグラフ化する

---

### 1.4 接待AIの設計実験

**目的：** 温度パラメータや最善手乖離度を調整し、意図的に弱い手を選ぶAIを作る

**実験手順：**
1. AI選択画面で任意のAIを選択
2. パラメータ調整画面で「温度パラメータ」または「最善手選択率」を調整
3. 対局を開始し、AIがどの程度「手加減」するか観察する

**実験パターン：**

#### パターンA：温度パラメータによる調整
- 温度: 0.1（低い）→ ほぼ最善手のみを選ぶ
- 温度: 1.0（標準）→ 評価値に応じた確率的選択
- 温度: 5.0（高い）→ ランダムに近い選択

**期待される変化：**
- 温度が高いほど、AIが「悪い手」を選ぶ確率が上がる
- 温度が低いほど、AIが「良い手」を選ぶ確率が上がる

#### パターンB：最善手選択率による調整
- 最善手選択率: 100% → 常に最善手を選ぶ
- 最善手選択率: 80% → 80%の確率で最善手、20%で2番目以降の手
- 最善手選択率: 50% → 50%の確率で最善手、50%で2番目以降の手

**期待される変化：**
- 選択率が低いほど、AIが「手加減」する
- プレイヤーが勝ちやすくなる
- しかし、時々強烈な手を繰り出す（「本気モード」）

**観察ポイント：**
- AIがどの程度「手加減」しているか
- プレイヤーが勝てるようになるか
- 手加減の度合いとゲームの楽しさの関係

**発展実験：**
- プレイヤーの実力に応じて最適な「手加減度」を探す
- 手加減AIと全力AIを交互に配置し、プレイヤーの学習効果を測定する

---

## 2. 技術解説（実験の裏付け）

### 2.1 探索深さ（ミニマックス法）

**設定項目：** 探索深さ（depth）

**技術的な説明：**
- ミニマックス法は、AIが何手先まで読むかを決定するパラメータ
- 探索深さが深いほど、AIは長期的な戦略を立てられる
- しかし、探索深さが1増えると計算量は指数関数的に増加する

**実験との関係：**
- 「近視眼AI」実験では、探索深さを極端に浅くすることで、AIの判断力の低下を体験できる
- 探索深さを段階的に増やすことで、AIの強さの変化を観察できる

**推奨設定：**
- 初心者向け: 2-3
- 中級者向け: 3-4
- 上級者向け: 4-5
- 最強: 6（MAX）

---

### 2.2 評価関数（駒の価値）

**設定項目：** 各駒の価値（pieceValues）

**技術的な説明：**
- 評価関数は、盤面の状態を数値化する関数
- 各駒に価値を設定することで、AIは「どの駒を守るべきか」「どの駒を取るべきか」を判断する
- 駒の価値は、ゲームの勝敗に直結する重要度を反映する

**標準設定（初期値）：**
```
ライオン: 10,000  // 取られると負け
にわとり: 600    // 成ったひよこ、強力
きりん: 400      // 前後左右に動ける
ぞう: 400        // 斜め4方向に動ける
ひよこ: 100      // 基本的な駒
```

**実験との関係：**
- 「偏愛AI」実験では、駒の価値を極端に変更することで、AIの行動パターンの変化を観察できる
- 駒の価値の比率が、AIの戦略に大きく影響することを理解できる

**調整のヒント：**
- ライオンの価値は常に最高に保つ（取られると負けのため）
- 他の駒の価値は、実際のゲームでの重要度に応じて調整
- 極端な設定（例：ひよこを10,000）は教育的効果が高いが、実用的ではない

---

### 2.3 モンテカルロ法（試行回数）

**設定項目：** 試行回数（simulations）

**技術的な説明：**
- モンテカルロ法は、ランダムなシミュレーションを繰り返し、最善手を探す手法
- 試行回数が多いほど、AIの判断は安定するが、計算時間も増加する
- 試行回数が少ないと、AIの手に「揺らぎ」が生じる

**実験との関係：**
- 「確率の気まぐれ」実験では、試行回数を変えることで、AIの思考の安定性の変化を観察できる
- 同じ局面で異なる手を選ぶ「気まぐれ」が、ゲームにどのような影響を与えるかを理解できる

**推奨設定：**
- 低試行回数（100-500回）: 思考が速いが不安定
- 中試行回数（1,000-5,000回）: バランスが良い
- 高試行回数（10,000回以上）: 思考が遅いが安定

---

### 2.4 温度パラメータ（ボルツマン分布）

**設定項目：** 温度（temperature）

**技術的な説明：**
- 温度パラメータは、AIが最善手以外の手を選ぶ確率を制御する
- 温度が高いほど、評価値の低い手も選ばれやすくなる
- 温度が低いほど、評価値の高い手のみが選ばれる

**数式（簡略版）：**
```
選択確率 = exp(評価値 / 温度) / Σ exp(評価値 / 温度)
```

**実験との関係：**
- 「接待AI」実験では、温度を調整することで、AIの「手加減度」を制御できる
- プレイヤーの実力に応じて、適切な温度を設定することで、楽しい対局ができる

**推奨設定：**
- 温度: 0.1 → ほぼ最善手のみ（本気モード）
- 温度: 1.0 → 標準的な選択（バランス型）
- 温度: 5.0 → ランダムに近い（手加減モード）

---

### 2.5 最善手選択率

**設定項目：** 最善手選択率（bestMoveRate）

**技術的な説明：**
- 最善手選択率は、AIが最善手を選ぶ確率を直接指定する方法
- 残りの確率では、2番目以降の手からランダムに選択
- 温度パラメータよりも直感的で、調整しやすい

**実験との関係：**
- 「接待AI」実験では、最善手選択率を下げることで、AIの「手加減度」を制御できる
- プレイヤーが勝てるように調整しながらも、時々強烈な手を繰り出す「本気モード」を実現できる

**推奨設定：**
- 100% → 常に最善手（本気モード）
- 80% → 時々手加減（中級者向け）
- 50% → 頻繁に手加減（初心者向け）

---

## 3. 実験結果の記録と分析

### 3.1 実験ノートの推奨フォーマット

```
実験名: [実験の名前]
日付: [実施日]
AIタイプ: [選択したAI]
パラメータ設定:
  - [パラメータ名]: [値]
  - [パラメータ名]: [値]

観察結果:
- [観察した内容1]
- [観察した内容2]

気づき:
- [気づいたこと1]
- [気づいたこと2]

次の実験:
- [次に試したいこと]
```

### 3.2 比較実験のすすめ

複数のパラメータ設定を比較することで、より深い理解が得られます。

**例：探索深さの比較**
- 探索深さ2のAI vs 探索深さ4のAI
- 同じ局面で、どちらのAIがより良い手を選ぶか
- 思考時間の違いを記録する

**例：駒の価値の比較**
- 標準設定のAI vs ひよこ偏愛AI
- どちらのAIがより強いか
- 戦略の違いを観察する

---

## 4. 教育的な活用方法

### 4.1 授業での活用例

**小学校高学年向け：**
- 「AIがどう考えているか」を体験する
- パラメータを変えると、AIの行動がどう変わるかを観察
- 簡単な実験（探索深さの変更など）を実施

**中学校向け：**
- 評価関数の概念を理解する
- 駒の価値を変更し、AIの行動パターンの変化を分析
- 実験結果をレポートにまとめる

**高校・大学向け：**
- ミニマックス法、モンテカルロ法などのアルゴリズムを理解
- パラメータ調整による最適化を体験
- 機械学習の基礎概念（評価関数、探索など）を学習

### 4.2 自習での活用例

- 各実験シナリオを順番に試す
- 実験ノートに結果を記録
- 自分なりの「最強AI」や「面白いAI」を作る
- 友達と対戦し、AIの強さを比較する

---

## 5. 実装状況と今後の拡張

### 5.1 現在実装済みの機能

- ✅ 探索深さの調整（ミニマックス法）
- ✅ 評価関数の駒の価値（固定値、将来的に調整可能に）
- ✅ ハイブリッドAI（ミニマックス + 評価関数）

### 5.2 今後実装予定の機能

- ⏳ 駒の価値の動的調整（UIから変更可能に）
- ⏳ モンテカルロ法の実装
- ⏳ 温度パラメータの実装
- ⏳ 最善手選択率の実装
- ⏳ 実験結果の記録機能
- ⏳ パラメータ設定の保存・読み込み機能

---

## 6. まとめ

この教育的コンテンツは、AIのパラメータ調整を通じて、以下の学習目標を達成することを目指します：

1. **AIの思考プロセスを理解する**
   - 探索深さ、評価関数、確率的選択などの概念を体験的に学習

2. **パラメータ調整の重要性を理解する**
   - パラメータを変えると、AIの行動がどう変わるかを観察

3. **実験的な思考を養う**
   - 仮説を立て、実験し、結果を分析するプロセスを体験

4. **ゲームAIの面白さを発見する**
   - 単に強いAIを作るだけでなく、面白いAIを作る楽しさを体験

実験シナリオを中心に据えることで、技術的な詳細に深入りせず、体験を通じてAIの仕組みを理解できる構成になっています。


